

# paths to the model and the audio
ENGINE_PATH_GPU="../MobileNetTens/Neural_Weight/Sources/mobileNet_engine.trt"
ENGINE_PATH_DLA="Sources/mobileNet_engine_DLA.trt"
AUDIO_PATH="../Resources/image_701x64.npy"
NB_REP=10
CLASSES="MobileNetTens/classes.npy"

# creation of the trt file if necessary :

if [ ! -e "$ENGINE_PATH_GPU" ]; then
      echo "\033[31mThe file mobileNet_engine.trt does not exist. Conversion in process …\033[0m “
    /usr/src/tensorrt/bin/trtexec --onnx=Sources/MobileNetTens_W_10.onnx --saveEngine=$ENGINE_PATH_GPU
fi

if [ ! -e "$ENGINE_PATH_DLA" ]; then
    echo "\033[31mThe file mobileNet_engine_DLA.trt does not exist. Conversion in process …\033[0m”
   /usr/src/tensorrt/bin/trtexec --onnx=Sources/MobileNetTens_W_10.onnx --useDLACore=0 --allowGPUFallback --saveEngine=$ENGINE_PATH_DLA
fi

# Change the power mode
sudo /usr/sbin/nvpmodel -m 0 # Mode MAXN
echo -e "\033[32m Running inference on GPU with Mode MAXN :  \033[0m"

# Run the inference
out_maxn_gpu=$(python3 Scripts/inference_mean_conso.py  \
    --engine_path=$ENGINE_PATH_GPU \
    --audio_path=$AUDIO_PATH \
    --engine=GPU \
    --nb_rep=$NB_REP \
    --classes_name=$CLASSES\
    --power_mode=MAXN)
    
result_maxn_gpu=$(echo "$out_maxn_gpu" | grep "* ")  
printf "$result_maxn_gpu"


printf "\n"  
echo -e "\033[32m Running inference on DLA with Mode MAXN :  \033[0m"
out_maxn_dla=$(python3 Scripts/inference_mean_conso.py  \
    --engine_path=$ENGINE_PATH_DLA \
    --audio_path=$AUDIO_PATH \
    --engine=DLA \
    --nb_rep=$NB_REP \
    --classes_name=$CLASSES\
    --power_mode=MAXN)

result_maxn_dla=$(echo "$out_maxn_dla" grep "*")   
printf $result_maxn_dla

printf "\n"
sudo /usr/sbin/nvpmodel -m 2 # Mode 15W
echo -e "\033[32m Running inference on GPU with Mode 15W :  \033[0m"
out_15w_gpu=$(python3 Scripts/inference_mean_conso.py  \
    --engine_path=$ENGINE_PATH_GPU \
    --audio_path=$AUDIO_PATH \
    --engine=GPU \
    --nb_rep=$NB_REP \
    --classes_name=$CLASSES\
    --power_mode=15W)

result_15_gpu=$(echo "$out_15w_gpu" | grep "*")   
printf $result_15_gpu    

printf "\n"
echo -e "\033[32m Running inference on DLA with Mode 15W :  \033[0m"
out_15w_dla=$(python3 Scripts/inference_mean_conso.py  \
    --engine_path=$ENGINE_PATH_DLA \
    --audio_path=$AUDIO_PATH \
    --engine=DLA \
    --nb_rep=$NB_REP \
    --classes_name=$CLASSES\
    --power_mode=15W)

result_15w_dla=$(echo "$out_15w_dla" | grep "*")   
printf $result_15_dla

printf "\n"
echo -e "\033[32m Running inference on GPU with Mode 30W :  \033[0m"
sudo /usr/sbin/nvpmodel -m 3 # Mode 30W ALL
out_30w_gpu=$(python3 Scripts/inference_mean_conso.py  \
    --engine_path=$ENGINE_PATH_GPU \
    --audio_path=$AUDIO_PATH \
    --engine=GPU \
    --nb_rep=$NB_REP \
    --classes_name=$CLASSES\
    --power_mode=30W)
    
result_30w_gpu=$(echo "$out_30w_gpu" | grep "*")   
printf $result_30w_gpu

printf "\n"
echo -e "\033[32m Running inference on DLA with Mode 30W :  \033[0m"
out_30w_dla=$(python3 Scripts/inference_mean_conso.py  \
    --engine_path=$ENGINE_PATH_DLA \
    --audio_path=$AUDIO_PATH \
    --engine=DLA \
    --nb_rep=$NB_REP \
    --classes_name=$CLASSES\
    --power_mode=30W)
    
result_30w_dla=$(echo "$out_30w_dla" | grep "*")  
printf $result_30w_dla

# Print the stats


printf "\n"
stat_maxn_gpu=$(echo "$out_maxn_gpu" | grep "|")
stat_maxn_dla=$(echo "$out_maxn_dla" | grep "|")
stat_30w_gpu=$(echo "$out_30w_gpu" | grep "|")
stat_30w_dla=$(echo "$out_30w_dla" | grep "|")
stat_15w_gpu=$(echo "$out_15w_gpu" | grep "|")
stat_15w_dla=$(echo "$out_15w_dla" | grep "|")

rows="%-1s %-10s| %-10s| %-10s| %-10s| %-10s | %-10s |\n"
printf "| %-10s| %-10s| %-10s| %-10s| %-10s | %-10s |\n" Mode Engine 'Delay (s)' 'GPU (mW)' 'CV (mW)' 'TOT (mW)'
printf "+-----------+-----------+-----------+-----------+------------+------------+ \n"
printf "$rows" $stat_maxn_gpu
printf "$rows" $stat_maxn_dla
printf "$rows" $stat_30w_gpu
printf "$rows" $stat_30w_dla
printf "$rows" $stat_15w_gpu
printf "$rows" $stat_15w_dla




