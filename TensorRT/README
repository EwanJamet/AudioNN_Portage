# -*- coding: utf-8 -*-
"""README.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aR5f6zxYVbN0_WKvdYs7yTqY7r6njasr

# Running Inferences and Measuring Time and Power Consumption on Jetson AGX XAVIER

This project is designed to run inferences using different power modes on the Jetson AGX Xavier platform. It includes scripts for running inference with PyTorch and TensorRT frameworks.

###Folders Structure

* **Pytorch**: Contains scripts and resources for running inferences using PyTorch framework.
* **TensorRT**: Contains scripts and resources for running inferences using TensorRT framework.
* **Resources**: Contains the input audio file as a spectre image.

### Both Pytorch and TensorRT folders have the same structure:

* **Results**: Contains folders for each run with the datetime in the name, where we save the results of each the inference: a csv file for power consumption and a png image that shows the data from the csv file.
* **Scripts**: Conatins the python codes that run the inference.

We have included an additional script called `without_infer.py`. This script  contains the same code as our main inference script, but it excludes the lines responsible for conducting the actual inference.
This is helpful to facilitate the precise measurement of power consumption only resulting form the inference process.

* **Sources**: Contains the classes python file, the `.onnx`original model. It is also usd to save the `.trt`engine produced after the conversion from onnx to trt framework in the bash file.

### Steps to Run



1.   Model Conversion: If not already done, the ONNX model is converted into the corresponding framework (PyTorch or TensorRT).
2.   Setting Up Paths: Define paths to the model and the audio files.
3.   Creating TensorRT Engine: If the TensorRT engine file does not exist, it is created from the ONNX model using trtexec.
4.    Running Inference: Inferences are run on both GPU and DLA (Deep Learning Accelerator) with different power modes.
5.    Measuring Performance: Performance metrics such as delay, GPU power consumption, CV (Computer Vision) power consumption, and total power consumption are measured and printed.

##Usage

### Prerequisites

*  NVIDIA Jetson AGX Xavier platform
*  PyTorch and TensorRT frameworks installed
*  Necessary audio and model files (ONNX format)

### Running TensorRT Inferences
Navigate to the TensorRT folder and execute the script trt_inference.txt
"""

bash trt_inference.txt

"""Since errors can occure when calling an internal DLA function several times, we advise you to call the script that doesn't use any DLA engine."""

bash trt_infer_woDLA.txt

"""###Ttr_inference.txt

The parameters needed for the inferences are defined at the begining of the script. The following parameters are defined :
- ENGINE_PATH_GPU : path to the trt engine running on the GPU.
- ENGINE_PATH_DLA : path to the trt engine running on DLA.
- AUDIO_PATH : path to the audio spectrogram.
- NB_REP : number of repetition the mean values are computed over.

This script runs the inference for different power modes and then print a table to compare the inference time and power consumption of all the inferences.

### Additional Notes

Power Modes: Different power modes (MAXN, 15W, 30W) are set using nvpmodel.
Results: Inference results including performance metrics are printed to the console.
"""
